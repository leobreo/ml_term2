{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compulsory Assignment 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leonard Brenk, Finn Federsan, Felix Wltschek"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csvs\n",
    "\n",
    "true_csv = pd.read_csv(\"True.csv\")\n",
    "fake_csv = pd.read_csv(\"Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(fake, true):\n",
    "    fake_csv = pd.read_csv(fake)\n",
    "    true__csv = pd.read_csv(true)\n",
    "    fake_csv['label'] = 0\n",
    "    true_csv['label'] = 1\n",
    "    df = pd.concat([fake_csv, true_csv], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and text, drop unused columns, clean punctuation, extra spaces, and make lowercase\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df['text'] = df['title'] + ' ' + df['text']\n",
    "    df = df.drop(['title', 'subject', 'date'], axis=1)\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", x))\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(\"\\s+\", \" \", x))\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Stoppwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and short words from the 'text' column in the DataFrame\n",
    "\n",
    "def remove_stops(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words and len(word) > 2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "def split_df(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data in train and test sets, pad sequences to a fixed length, and determine the vocabulary size\n",
    "\n",
    "def tok_pad_df(X_train, X_test, max_len=250):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    return X_train_padded, X_test_padded, vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM model with embedding layer, two bidirectional LSTM layers, dense layers with Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "\n",
    "def build_model(vocab_size, max_len=250):\n",
    "    model = Sequential() # for adding the layers \n",
    "    model.add(Embedding(vocab_size, 128, input_length=max_len)) # turns tokens into layers of vectors \n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True))) # bidirectional to learn from both future and past \n",
    "    model.add(Bidirectional(LSTM(64))) #\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, batch_size=64, epochs=2):\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data, print the accuracy, and return the loss and accuracy values\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return loss, accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "449/449 [==============================] - 70s 148ms/step - loss: 0.0964 - accuracy: 0.9603 - val_loss: 0.0231 - val_accuracy: 0.9926\n",
      "Epoch 2/2\n",
      "449/449 [==============================] - 63s 140ms/step - loss: 0.0068 - accuracy: 0.9984 - val_loss: 0.0281 - val_accuracy: 0.9905\n",
      "281/281 [==============================] - 12s 43ms/step - loss: 0.0339 - accuracy: 0.9885\n",
      "Model Accuracy: 98.85%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03386697918176651, 0.9885300993919373)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "fake_file = 'Fake.csv'\n",
    "true_file = 'True.csv'\n",
    "df = load_datasets(fake_file, true_file)\n",
    "df.head()  \n",
    "\n",
    "# Preprocessing\n",
    "preprocessed_df = preprocess_df(df)\n",
    "preprocessed_df.head()  \n",
    "\n",
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = split_df(preprocessed_df)\n",
    "\n",
    "# Tokenizing and padding (all inputs same sized)\n",
    "X_train_padded, X_test_padded, vocab_size = tok_pad_df(X_train, X_test)\n",
    "\n",
    "# Building model\n",
    "model = build_model(vocab_size)\n",
    "\n",
    "# Training model\n",
    "train_model(model, X_train_padded, y_train)\n",
    "\n",
    "# Evaluating\n",
    "evaluate_model(model, X_test_padded, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
